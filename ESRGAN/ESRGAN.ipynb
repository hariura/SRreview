{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a1af78",
   "metadata": {},
   "source": [
    "# ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafbafd",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbuEqNc%2FbtrqN2L0EbI%2FbpM7c6SG3ewk5AOxhLHY40%2Fimg.png\">\n",
    "baboon : 개코원숭이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5303e",
   "metadata": {},
   "source": [
    "- Three key components of SRGAN\n",
    "    - Network architecture (RRDB)\n",
    "    - Adversarial loss (idea from relativstic GAN)\n",
    "    - Perceptual loss ( by using the features before activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e8bff",
   "metadata": {},
   "source": [
    "### Proposed Methods\n",
    "##### MAIN AIM : To improve the overall perceptual quality for SR \n",
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e283d",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMn9n7%2FbtrqOgjBfNo%2FBsqG1joBXE4Wfcs7gPH5j1%2Fimg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93968c5e",
   "metadata": {},
   "source": [
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FthFaB%2FbtrqNLRE0Va%2FydtkURqs9B0Ni4tQKRiYxk%2Fimg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ffe8b",
   "metadata": {},
   "source": [
    "- We employ the basic architecture of SRResNet\n",
    "- Left: We remove the BN layers in residual block in SRGAN.\n",
    "- Right: RRDB block. $\\beta$ is the residual scaling parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ed526",
   "metadata": {},
   "source": [
    "##### Removing Batch Nomalize layers\n",
    "- When the statistics of training and testing datasets diﬀer a lot, **BN layers tend to introduce unpleasant artifacts and limit the generalization ability.**\n",
    "- Furthermore, removing BN layers helps to improve generalization ability and to reduce computational complexity and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae92a24",
   "metadata": {},
   "source": [
    "- Based on the observation that more layers and connections could always boost performance, the proposed RRDB employs a **deeper and more complex structure** than the original residual block in SRGAN.  \n",
    "\n",
    "\n",
    "- Exploit several techniques\n",
    "    - 1) Residual scaling : multiplying a constant $\\beta$ between 0 and 1\n",
    "    - 2) Smaller initialization : supplementary material.   \n",
    "    \n",
    "        - (we empirically ﬁnd residual architecture is easier to train when the initial parameter variance becomes smaller.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b67c7f",
   "metadata": {},
   "source": [
    "### Relativistic Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c0e16",
   "metadata": {},
   "source": [
    "- Enhance the discriminator based on the Relativistic GAN\n",
    "- A relativistic discriminator tries to predict the probability that a real image $x_r$is relatively more realistic than a fake one $x_f$\n",
    "\n",
    "<img src= \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdGCpa0%2FbtrqN2LYSeE%2FFqwx2bud9hd0FeWx2eD2V0%2Fimg.png\">\n",
    "\n",
    "- Relativistic average Discriminator RaD [2], denoted as $D_{Ra}$\n",
    "- $\\sigma$ is the sigmoid function and $C(x)$ is the non-transformed discriminator output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0040616",
   "metadata": {},
   "source": [
    "#### Discriminator loss\n",
    "$$L_D^{Ra} = - \\mathbb{E}_{x_r}[\\log(D_{Ra}(x_r, x_f))]- \\mathbb{E}_{x_f}[1- \\log(D_{Ra}(x_f, x_r))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171e8dc",
   "metadata": {},
   "source": [
    "#### Adversarial loss for generator\n",
    "$$L_G^{Ra} = - \\mathbb{E}_{x_r}[1- \\log(D_{Ra}(x_r, x_f))]- \\mathbb{E}_{x_f}[\\log(D_{Ra}(x_f, x_r))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da519e3",
   "metadata": {},
   "source": [
    "where $x_f = G(x_i), x_i $ : input LR image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8661ae",
   "metadata": {},
   "source": [
    "### Perceptual Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5dfe71",
   "metadata": {},
   "source": [
    "- Use features before the activation layers\n",
    "    - First, the activated features are very sparse (Fig.)\n",
    "    - Second, using features after activation also causes inconsistent reconstructed brightness compared with the ground-truth  (Sec. 4.4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c535e",
   "metadata": {},
   "source": [
    "**The total loss for the generator**\n",
    "$$L_G = L_{percep} + \\lambda L_G^{Ra} + \\eta L_1$$\n",
    "where $L_1 = \\mathbb{E}_{x_i}||G(x_i) - y||_1$  \n",
    "$\\lambda, \\eta :$ coefficient to balance different loss term\n",
    "($\\lambda= 5×10^{−3}, η = 1×10^{−2}$ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ff4ec",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb0TySQ%2FbtrqRTuGsGE%2FOlNBJNgK1yoHHg32kXeUWK%2Fimg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580979b",
   "metadata": {},
   "source": [
    "### Network Interpolation\n",
    "- we ﬁrst train a PSNR-oriented network $G_{PSNR}$and then obtain a GAN-based network $G_{GAN}$ by ﬁne-tuning.\n",
    "\n",
    "#### interpolated model $G_{INTERP}$\n",
    "$$\\theta_{G}^{INTERP} = (1-\\alpha)\\theta_G^{PSNR} + \\alpha \\theta_G^{GAN}$$\n",
    "  \n",
    "where $\\theta_{G}^{INTERP}, \\theta_G^{PSNR}, \\theta_G^{GAN}$ : parameter $ {G}^{INTERP}, G^{PSNR}, G^{GAN}$  \n",
    "$\\alpha = [0,1]$\n",
    "\n",
    "#### Network interpolation enjoys two merits.\n",
    "- First, the interpolated model is able to produce meaningful results for any feasible $\\alpha$ without introducing artifacts.\n",
    "- Second, we can continuously balance perceptual quality and ﬁdelity **without re-training the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429f4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
