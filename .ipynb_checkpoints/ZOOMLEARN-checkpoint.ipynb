{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a53982",
   "metadata": {},
   "source": [
    "# Zoom to Learn, Learn to Zoom\n",
    "#### Xuaner Zhang\n",
    "#### 2019 CVPR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b9389",
   "metadata": {},
   "source": [
    "## Key idea\n",
    "- Raw Sensor data\n",
    "- Optical magnification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659bae7",
   "metadata": {},
   "source": [
    "<img src=\"https://ceciliavision.github.io/images/project-page/cvpr19.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461b2b5",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "- This paper shows that when applying machine learning to __digital zoom__, it is beneficial to operate on real, **RAW sensor data**\n",
    "- Existing __learning-based super-resolution methods__ do not use real sensor data, instead operating on processed **RGB images**.\n",
    "- Ground-truth data via optical zoom and contribute a dataset, SR-RAW\n",
    "- **Novel contextual bilateral loss** that is robust to mild misalignment between input and outputs images\n",
    "- Synthesizing sensor data by resampling high-resolution RGB images is an oversimplified approximation of **real sensor data and noise, resulting in worse image quality**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5ca43f",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "- We propose to improve the quality of super-resolution by starting with real raw sensor data.\n",
    "- Recently, single image super-resolution has progressed with deep models and learned image priors from large-scale datasets\n",
    "- these methods are **constrained**\n",
    "    - the input image is a downsampled version of the high-resolution image, indirectly reducing the noise level in the input.\n",
    "    - 8-bit RGB image that has been processed by the camera’s image signal processor (ISP), which trades off high-frequency signal in higher-bit raw sensor data for other objectives  \n",
    "    \n",
    "    \n",
    "-  The **fundamental challenge** is obtaining ground truth for this task\n",
    "    - One approach is to synthesize sensor data from 8-bit RGB images that are passed through some synthetic noise model\n",
    "    - The reason is that sensor noise comes from a variety of sources  \n",
    "\n",
    "    \n",
    "- To enable learning from real raw sensor data for better computational zoom, we propose to **capture real data with a zoom lens**\n",
    "- SR-RAW contains ground-truth high-resolution images taken with high optical zoom levels.\n",
    "  \n",
    "  \n",
    "- SR-RAW brings up a new challenge : different camera configurations\n",
    "- Mildly misaligned input-output image pairs make pixel-wise loss functions unsuitable for training\n",
    "- We thus introduce a novel contextual bilateral loss (CoBi) that is robust to such mild misalignment\n",
    "    - CoBi draws inspiration from the recently proposed contextual loss (CX)\n",
    "\n",
    "### Our contributions\n",
    "- We demonstrate **the utility of using real high-bit sensor data for computational zoom, rather than processed 8-bit RGB images or synthetic sensor models.**\n",
    "- **SR-RAW, the first dataset for super-resolution from raw data, with optical ground truth**\n",
    "- We propose a **novel contextual bilateral loss (CoBi)** that handles slightly misaligned image pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa3ac3",
   "metadata": {},
   "source": [
    "## 2. Related Work \n",
    "- Image super-resolution has advanced from traditional ﬁltering to learning-based methods.\n",
    "    - Recently, deep neural networks have been applied to super-resolution, trained with a **variety of losses**\n",
    "- GAN : RGB as input \n",
    "    - SRGAN, ESRGAN,LapSRN\n",
    "- Image Processing with Raw Data : synthetic bayer mosaics \n",
    "    - propose method to super resolve image by jointly optimizing for the camera image processing pipeline and super-resolution from raw sensor data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0333588e",
   "metadata": {},
   "source": [
    "## 3. Dataset With Optical Zoom Sequences\n",
    "\n",
    "- SR-RAW contains raw sensor data and **ground-truth high-resolution images** taken with a zoom lens at **various zoom levels**\n",
    "- For data preprocessing, we **align** the captured images with different zoom levels via field of view (FOV) matching and geometric transformation. \n",
    "\n",
    "\n",
    "<img src=\"./figure2_1.png\">\n",
    "\n",
    "### 3.1. Data Capture with a Zoom Lens\n",
    "- Each pair of images forms an input-output pair for training a model: the short-focal-length raw sensor image is used as input and the long-focal-length RGB image is regarded as the groundtruth for super-resolution.\n",
    "\n",
    "    - For example, the RGB image taken with a 70mm focal length serves as the 2X zoom ground truth for the raw sensor data taken with a 35mm focal length.\n",
    "    - we collect 7 images under 7 optical zoom settings per scene for data collection efficiency\n",
    "- In total, we collect 500 sequences in indoor and outdoor scenes. ISO ranges from 100 to 400.\n",
    "  \n",
    "  \n",
    "- During data capture, camera settings are important.\n",
    "     - First, depth of field (DOF) changes with focal length and it is **not practical** to adjust aperture size for each focal length to make DOF identical.\n",
    "     - We choose a small aperture size (at least f/20) to minimize the DOF difference (still notice- able in Figure 2 B2), using a tripod to capture indoor scenes with a long exposure time. \n",
    "     - Second, we use **the same exposure time for all images in a sequence** so that noise level is not affected by focal length change.\n",
    "     - But we still observe noticeable illumination variations. This color variation is another motivation for us to **avoid using pixel-to-pixel losses for training.**\n",
    "     \n",
    "### 3.2. Data Preprocessing\n",
    "- RGB-L, RAW-L, RGB-H, RAW-H, and FOV(시야각)\n",
    "- We apply a Euclidean motion model that allows image rotation and translation via enhanced correlation coefficient minimization.\n",
    "- A scale offset is applied to the image if the optical zoom does not perfectly match the target zoom ratio.\n",
    "\n",
    "\n",
    "### 3.3. Misalignment Analysis\n",
    "- Misalignment is unavoidable during data capture and can hardly be eliminated by the preprocessing step.\n",
    "-  misalignment is inherently caused by the perspective changes\n",
    "- The described misalignment in SR-RAW usually causes **40-80 pixel shifts in an 8-megapixel image pair.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adfa88c",
   "metadata": {},
   "source": [
    "## 4. Contextual Bilateral Loss\n",
    "- The contextual loss was proposed to train with unaligned data pairs.\n",
    "- Source image P: feature point ${p_{i}}_{i=1}^{N}$ and target image Q : feature point ${q_{j}}_{j=1}^{M}$  \n",
    "- For each source image feature $p$, it searches for the nearest neighbor (NN) feature match $q$ such that $q = arg\\min_{q}\\mathbb{D}(p,q)_{j=1}^{M}$ under some distance measure $\\mathbb{D}(p,q)$\n",
    "- Given input image P and its target Q, the contextual loss tries to minimize the summed distance of all matched feature pairs, formulated as\n",
    "$$\\mathrm{CX}(P,Q) = \\frac{1}{N}\\sum_{i}^N \\min_{j=1,\\ldots,M}(\\mathbb{D}_{p_{i},q_{j}})$$\n",
    "- We hypothesize that these artifacts are caused by inaccurate feature matching in the contextual loss.\n",
    "- We thus analyze the percentage of features that are matched uniquely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b60a5",
   "metadata": {},
   "source": [
    "- Inspired by the edge-preserving bilateral ﬁlter [28], we integrate the spatial pixel coordinates and pixel-level RGB information into the image features.\n",
    "$$\\mathrm{CoBi}(P,Q)= \\frac{1}{N}\\sum_{i}^N \\min_{j=1,\\ldots,M}(\\mathbb{D}_{p_{i},q_{j}} + w_s\\mathbb{D}'_{p_{i},q_{j}})$$\n",
    "where $\\mathbb{D}'_{p_{i},q_{j}} = ||(x_i, y_i) - (x_j,y_j)||_2$  \n",
    "\n",
    "- $(x_i,y_i)$ and $(x_j,y_j)$ are spatial coordinates of feature $p_i$ and $q_j$\n",
    "- $w_s$ denote the the weight of spatial awareness for nearest neighbor search.\n",
    "- we use pretrained VGG-19 feature and select 'conv1_2', \"conv2_2\" and \"conv3_2\" as our deep feature\n",
    "- Cosine distance is used to measure feature similarity. $\\mathrm{similarity}(x,y) = \\cos(\\theta) = \\frac{x \\cdot y}{|x||y|}$\n",
    "\n",
    "#### Our ﬁnal loss function is deﬁned as\n",
    "\n",
    "$$\\mathrm{CoBi}_{\\mathrm{RGB}}(P,Q,n) + \\lambda \\mathrm{CoBi}_{\\mathrm{VGG}}(P,Q)$$\n",
    "\n",
    "where we use $n \\times n$ RGB patches as features for $\\mathrm{CoBi}_{RGB}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae3d6e",
   "metadata": {},
   "source": [
    "## 5. Experimental Setup\n",
    "- We pack each 2 × 2 block in the raw Bayer mosaic into 4 channels as input for our model. The packing reduces the spatial resolution of the image by a factor of two in width and height, without any loss of signal.\n",
    "- We adopt a 16-layer ResNet architecture followed by $\\log_{2} N + 1$ up-convolution layers where N is the zoom factor.\n",
    "- We split 500 sequences in SR-RAW into training, validation, and test sets with a ratio of 80:10:10 \n",
    "- so training : 400, validation : 50,  test : 50 \n",
    "- For a 4X zoom model, we get 3 input-output pairs per sequence for training, and for an 8X zoom model, we get 1 pair per sequence.\n",
    "- We randomly crop 64 × 64 patches from a full-resolution Bayer mosaic as input for training.\n",
    "\n",
    "### 5.1. Baselines\n",
    "- SRGAN, SRResnet, LapSRN and ESRGAN\n",
    "- we ﬁrst try to ﬁne-tune their models on SR-RAW. However, we notice little difference in average performance\n",
    "\n",
    "### 5.2. Controlled Experiments on Our Model\n",
    "\n",
    "- “Ours-png”: For comparison, we also train a copy of our model (“Ours-png”) using 8-bit processed RGB images to evaluate the beneﬁts of having real raw sensor data.\n",
    "- “Ours-syn-raw”. To test whether synthesized raw data can replace real sensor data for training,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31703b29",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "- To quantitatively evaluate the presented approach, we use the standard **SSIM** and **PSNR** metrics, as well as the recently proposed learned perceptual metric **LPIPS**\n",
    "- GAN-based methods often generate noisy artifacts and lead to low PSNR and SSIM scores\n",
    "- Bicubic upsampling and SRResnet produce blurry results and get a low score in LPIPS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5af2f",
   "metadata": {},
   "source": [
    "<img src=\"./table_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38aa5ce",
   "metadata": {},
   "source": [
    "<img src=\"./figure_5.png\">\n",
    "<img src=\"./figure_6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1494ece",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
